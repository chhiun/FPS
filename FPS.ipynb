{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3121a96-5bdd-4e44-9f27-e1475cc59dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chhiun\\miniconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MovenetProcessor:\n",
    "    def __init__(self, video_path, output_path, model_path=\"movenet-tensorflow2-multipose-lightning-v1\"):\n",
    "        self.video_path = video_path\n",
    "        self.output_path = output_path\n",
    "        self.model_path = model_path\n",
    "        self.confidence_threshold = 0.1\n",
    "        self.seconds_to_extract = None\n",
    "        self.input_size = 384\n",
    "\n",
    "        # Check if GPU is available\n",
    "        self._setup_gpu()\n",
    "\n",
    "        # Load the MoveNet model\n",
    "        self.model = hub.load(model_path)\n",
    "        self.movenet = self.model.signatures['serving_default']\n",
    "        self.EDGES = {\n",
    "            (0, 1): 'm',    # Top of the head - Left eye\n",
    "            (0, 2): 'c',    # Top of the head - Right eye\n",
    "            (1, 3): 'm',    # Left eye - Left ear\n",
    "            (2, 4): 'c',    # Right eye - Right ear\n",
    "            (0, 5): 'm',    # Top of the head - Left shoulder\n",
    "            (0, 6): 'c',    # Top of the head - Right shoulder\n",
    "            (5, 7): 'm',    # Left shoulder - Left elbow\n",
    "            (7, 9): 'm',    # Left elbow - Left wrist\n",
    "            (6, 8): 'c',    # Right shoulder - Right elbow\n",
    "            (8, 10): 'c',   # Right elbow - Right wrist\n",
    "            (5, 6): 'y',    # Left shoulder - Right shoulder\n",
    "            (5, 11): 'm',   # Left shoulder - Left hip\n",
    "            (6, 12): 'c',   # Right shoulder - Right hip\n",
    "            (11, 12): 'y',  # Left hip - Right hip\n",
    "            (11, 13): 'm',  # Left hip - Left knee\n",
    "            (13, 15): 'm',  # Left knee - Left ankle\n",
    "            (12, 14): 'c',  # Right hip - Right knee\n",
    "            (14, 16): 'c'   # Right knee - Right ankle\n",
    "        }\n",
    "\n",
    "        # Warm up the model\n",
    "        self._warm_up_model()\n",
    "\n",
    "    def _setup_gpu(self):\n",
    "        physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "            tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n",
    "            print(f\"Using GPU: {physical_devices[0]}\")\n",
    "        else:\n",
    "            print(\"No GPU found, using CPU.\")\n",
    "\n",
    "    def _warm_up_model(self):\n",
    "        print(\"Warming up model...\")\n",
    "        # Create a dummy image to warm up the model\n",
    "        warm_up_image = tf.zeros([1, self.input_size, self.input_size, 3], dtype=tf.int32)\n",
    "        _ = self.movenet(warm_up_image)\n",
    "        print(\"Model is warmed up.\")\n",
    "\n",
    "    def set_params(self, confidence_threshold=None, seconds_to_extract=None, input_size=None):\n",
    "        if confidence_threshold is not None:\n",
    "            self.confidence_threshold = confidence_threshold\n",
    "        if seconds_to_extract is not None:\n",
    "            self.seconds_to_extract = seconds_to_extract\n",
    "        if input_size is not None:\n",
    "            if input_size % 32 != 0:\n",
    "                raise ValueError(\"input_size must be a multiple of 32\")\n",
    "            self.input_size = input_size\n",
    "            self._warm_up_model() # Warm up the model again if input size changes\n",
    "\n",
    "    def process_input(self, image):\n",
    "        image = tf.convert_to_tensor(image, dtype=tf.int32)\n",
    "        original_height, original_width, _ = image.shape\n",
    "        aspect_ratio = original_width / original_height\n",
    "\n",
    "        # Calculate the new width while maintaining the aspect ratio and ensuring it's a multiple of 32\n",
    "        new_width = int(self.input_size * aspect_ratio)\n",
    "        new_width = (new_width // 32) * 32 # Ensure the new_width is a multiple of 32\n",
    "\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        image = tf.image.resize_with_pad(image, self.input_size, new_width)# 1440*2560 / 4 or 2\n",
    "        image = tf.cast(image, dtype=tf.int32)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def draw_keypoints(self, frame, keypoints):\n",
    "        y, x, _ = frame.shape\n",
    "        shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "        head_points = []\n",
    "        for i, kp in enumerate(shaped):\n",
    "            ky, kx, kp_conf = kp\n",
    "            if kp_conf > self.confidence_threshold:\n",
    "                if i in [0, 1, 2, 3, 4]:\n",
    "                    head_points.append((kx, ky))\n",
    "                cv2.circle(frame, (int(kx), int(ky)), 3, (0, 255, 0), -1)\n",
    "\n",
    "        return head_points\n",
    "\n",
    "    def draw_head_box(self, frame, head_points):\n",
    "        if head_points:\n",
    "            x_coordinates = [int(pt[0]) for pt in head_points]\n",
    "            y_coordinates = [int(pt[1]) for pt in head_points]\n",
    "            min_x, max_x = min(x_coordinates), max(x_coordinates)\n",
    "            min_y, max_y = min(y_coordinates), max(y_coordinates)\n",
    "            cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)\n",
    "\n",
    "    def draw_connections(self, frame, keypoints):\n",
    "        y, x, _ = frame.shape\n",
    "        shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "\n",
    "        for edge, _ in self.EDGES.items():\n",
    "            p1, p2 = edge\n",
    "            y1, x1, c1 = shaped[p1]\n",
    "            y2, x2, c2 = shaped[p2]\n",
    "\n",
    "            if (c1 > self.confidence_threshold) & (c2 > self.confidence_threshold):\n",
    "                cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 4)\n",
    "\n",
    "    def loop_through_people(self, frame, keypoints_with_scores):\n",
    "        for person in keypoints_with_scores:\n",
    "            self.draw_connections(frame, person)\n",
    "            head_points = self.draw_keypoints(frame, person)\n",
    "            self.draw_head_box(frame, head_points)\n",
    "\n",
    "    def process_video(self):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(self.output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "        if self.seconds_to_extract is not None:\n",
    "            frames_to_process = min(fps * self.seconds_to_extract, total_frames)\n",
    "        else:\n",
    "            frames_to_process = total_frames\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=frames_to_process, desc=\"Processing Video\", leave=False)\n",
    "\n",
    "        while frame_count < frames_to_process:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_image = self.process_input(rgb_frame)\n",
    "\n",
    "            outputs = self.movenet(input_image)\n",
    "            keypoints_with_scores = outputs['output_0'].numpy()[:, :, :51].reshape((6, 17, 3))\n",
    "\n",
    "            self.loop_through_people(frame, keypoints_with_scores)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Processing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0248a225-cb5b-4469-9606-767cc70188fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU.\n",
      "WARNING:tensorflow:From C:\\Users\\chhiun\\miniconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:498: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\chhiun\\miniconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "Warming up model...\n",
      "Model is warmed up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "processor = MovenetProcessor('20240402_200836.mp4', 'result.mp4')\n",
    "processor.set_params(seconds_to_extract=None, confidence_threshold=0.25)\n",
    "processor.process_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242e823-2c18-4ee3-adaf-3257e7550325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
